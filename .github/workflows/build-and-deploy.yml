name: Build, Push and Deploy

on:
  # Push'ta otomatik Ã§alÄ±ÅŸÄ±r
  push:
    branches: [main, master]
  # Manuel de Ã§alÄ±ÅŸtÄ±rÄ±labilir
  workflow_dispatch:
    inputs:
      action:
        description: 'What to do'
        required: true
        default: 'build-and-deploy'
        type: choice
        options:
          - build-only
          - deploy-only
          - build-and-deploy

env:
  REGISTRY: docker.io
  DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
  ENVIRONMENT: production
  TF_VERSION: '1.5.0'
  TF_WORKING_DIR: './terraform'

jobs:
  # ============================================================================
  # JOB 0: Terraform Apply
  # ============================================================================
  terraform:
    name: 'Terraform Apply'
    runs-on: ubuntu-latest
    environment: production
    if: github.event_name == 'push' || github.event.inputs.action != 'deploy-only'

    defaults:
      run:
        working-directory: ${{ env.TF_WORKING_DIR }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}

      - name: Terraform Init
        run: terraform init
        env:
          HCLOUD_TOKEN: ${{ secrets.HCLOUD_TOKEN }}

      - name: Terraform Import Existing Resources
        run: |
          set -e
          if [ -n "${HCLOUD_NETWORK_ID:-}" ]; then
            terraform state show hcloud_network.main >/dev/null 2>&1 || \
              terraform import \
                -var="hcloud_token=${{ secrets.HCLOUD_TOKEN }}" \
                -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
                -var-file="terraform-simple.tfvars" \
                hcloud_network.main "${HCLOUD_NETWORK_ID}"
          fi

          if [ -n "${HCLOUD_FIREWALL_ID:-}" ]; then
            terraform state show hcloud_firewall.cluster >/dev/null 2>&1 || \
              terraform import \
                -var="hcloud_token=${{ secrets.HCLOUD_TOKEN }}" \
                -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
                -var-file="terraform-simple.tfvars" \
                hcloud_firewall.cluster "${HCLOUD_FIREWALL_ID}"
          fi

          if [ -n "${HCLOUD_NETWORK_ID:-}" ]; then
            if ! terraform state show hcloud_network_subnet.kubernetes >/dev/null 2>&1; then
              terraform import \
                -var="hcloud_token=${{ secrets.HCLOUD_TOKEN }}" \
                -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
                -var-file="terraform-simple.tfvars" \
                hcloud_network_subnet.kubernetes "${HCLOUD_NETWORK_ID}:10.0.1.0/24" || \
              terraform import \
                -var="hcloud_token=${{ secrets.HCLOUD_TOKEN }}" \
                -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
                -var-file="terraform-simple.tfvars" \
                hcloud_network_subnet.kubernetes "${HCLOUD_NETWORK_ID}-10.0.1.0/24"
            fi
          fi

          if [ -n "${HCLOUD_CONTROL_PLANE_ID:-}" ]; then
            terraform state show hcloud_server.control_plane >/dev/null 2>&1 || \
              terraform import \
                -var="hcloud_token=${{ secrets.HCLOUD_TOKEN }}" \
                -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
                -var-file="terraform-simple.tfvars" \
                hcloud_server.control_plane "${HCLOUD_CONTROL_PLANE_ID}"
          fi

          if [ -n "${HCLOUD_WORKER_IDS:-}" ]; then
            IFS=',' read -r -a WORKER_IDS <<< "${HCLOUD_WORKER_IDS}"
            for i in "${!WORKER_IDS[@]}"; do
              WORKER_ID="${WORKER_IDS[$i]}"
              if [ -n "$WORKER_ID" ]; then
                terraform state show "hcloud_server.workers[$i]" >/dev/null 2>&1 || \
                  terraform import \
                    -var="hcloud_token=${{ secrets.HCLOUD_TOKEN }}" \
                    -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
                    -var-file="terraform-simple.tfvars" \
                    "hcloud_server.workers[$i]" "${WORKER_ID}"
              fi
            done
          fi
        env:
          HCLOUD_NETWORK_ID: ${{ secrets.HCLOUD_NETWORK_ID }}
          HCLOUD_FIREWALL_ID: ${{ secrets.HCLOUD_FIREWALL_ID }}
          HCLOUD_CONTROL_PLANE_ID: ${{ secrets.HCLOUD_CONTROL_PLANE_ID }}
          HCLOUD_WORKER_IDS: ${{ secrets.HCLOUD_WORKER_IDS }}

      - name: Terraform Apply
        run: |
          terraform apply -auto-approve \
            -var="hcloud_token=${{ secrets.HCLOUD_TOKEN }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var-file="terraform-simple.tfvars"

  # ============================================================================
  # JOB 1: Build Images
  # ============================================================================
  build:
    name: 'Build Docker Images'
    runs-on: ubuntu-latest
    # Push'ta veya manuel build-and-deploy/build-only seÃ§ildiÄŸinde Ã§alÄ±ÅŸÄ±r
    if: github.event_name == 'push' || github.event.inputs.action != 'deploy-only'
    needs: [terraform]
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Build and Push auth-api
        uses: docker/build-push-action@v5
        with:
          context: ./auth-api
          push: true
          tags: |
            ${{ secrets.DOCKER_USERNAME }}/auth-api:${{ github.sha }}
            ${{ secrets.DOCKER_USERNAME }}/auth-api:latest

      - name: Build and Push product-api
        uses: docker/build-push-action@v5
        with:
          context: ./product-api
          push: true
          tags: |
            ${{ secrets.DOCKER_USERNAME }}/product-api:${{ github.sha }}
            ${{ secrets.DOCKER_USERNAME }}/product-api:latest

      - name: Build and Push gateway
        uses: docker/build-push-action@v5
        with:
          context: ./gateway
          push: true
          tags: |
            ${{ secrets.DOCKER_USERNAME }}/gateway:${{ github.sha }}
            ${{ secrets.DOCKER_USERNAME }}/gateway:latest

      - name: Build and Push ecommerce
        uses: docker/build-push-action@v5
        with:
          context: ./ecommerce
          push: true
          tags: |
            ${{ secrets.DOCKER_USERNAME }}/ecommerce:${{ github.sha }}
            ${{ secrets.DOCKER_USERNAME }}/ecommerce:latest

  # ============================================================================
  # JOB 2: Deploy to k3s (Build tamamlandÄ±ktan sonra)
  # ============================================================================
  deploy:
    name: 'Deploy to k3s'
    runs-on: ubuntu-latest
    needs: [build]  # Build bittikten sonra baÅŸlar
    if: always() && needs.build.result == 'success'
    env:
      DEPLOY_ENV: ${{ github.event.inputs.environment || 'production' }}
      KUBECONFIG: /home/runner/.kube/config
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: '3.13.0'

      - name: Setup SSH and Kubeconfig
        run: |
          set -euo pipefail
          mkdir -p ~/.ssh ~/.kube
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_ed25519
          chmod 600 ~/.ssh/id_ed25519
          
          MASTER_IP="${{ secrets.MASTER_IP }}"
          ssh-keyscan -H $MASTER_IP >> ~/.ssh/known_hosts 2>/dev/null || true
          
          # Download kubeconfig from master
          ssh -o StrictHostKeyChecking=no root@$MASTER_IP \
            "cat /etc/rancher/k3s/k3s.yaml" | \
            sed "s/127.0.0.1/$MASTER_IP/g" > ~/.kube/config
          
          chmod 600 ~/.kube/config
          if [ ! -s ~/.kube/config ]; then
            echo "Kubeconfig yazilamadi veya bos. SSH ve MASTER_IP bilgilerini kontrol edin."
            exit 1
          fi

      - name: Verify cluster connection
        run: |
          kubectl cluster-info
          kubectl get nodes
          # API server baÄŸlantÄ±sÄ±nÄ± test et
          kubectl get namespaces --request-timeout=30s
          
      - name: Check Storage Classes
        run: |
          echo "ðŸ“¦ Mevcut Storage Classes:"
          kubectl get storageclass || echo "Storage class bulunamadÄ±"
          echo ""
          echo "VarsayÄ±lan storage class:"
          kubectl get storageclass -o jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubernetes\.io/is-default-class=="true")].metadata.name}' || echo "VarsayÄ±lan storage class yok"
          
      - name: Check Cluster Resources
        run: |
          echo "## ðŸ“Š Cluster Kaynak Durumu" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Node KaynaklarÄ±:" >> $GITHUB_STEP_SUMMARY
          kubectl top nodes 2>/dev/null >> $GITHUB_STEP_SUMMARY || echo "Metrics server yok, node kaynaklarÄ± gÃ¶sterilemiyor" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Node DetaylarÄ±:" >> $GITHUB_STEP_SUMMARY
          kubectl describe nodes | grep -A 5 "Allocated resources" >> $GITHUB_STEP_SUMMARY || true
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "ðŸ” Cluster kaynaklarÄ± kontrol ediliyor..."
          kubectl get nodes -o custom-columns=NAME:.metadata.name,CPU:.status.capacity.cpu,MEMORY:.status.capacity.memory

      - name: Create namespace
        run: |
          kubectl create namespace ${{ env.DEPLOY_ENV }} --dry-run=client -o yaml | kubectl apply -f -

      - name: Create Docker Registry Secret
        run: |
          kubectl create secret docker-registry regcred \
            --docker-server=docker.io \
            --docker-username=${{ secrets.DOCKER_USERNAME }} \
            --docker-password=${{ secrets.DOCKER_PASSWORD }} \
            --namespace=${{ env.DEPLOY_ENV }} \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Helm Dependency Update
        run: |
          cd aws-k8s-helm
          helm dependency update

      - name: Install/Upgrade Ingress NGINX
        run: |
          helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
          helm repo update
          helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
            --namespace ingress-nginx \
            --create-namespace \
            --set controller.service.type=NodePort \
            --set controller.service.nodePorts.http=30080 \
            --set controller.service.nodePorts.https=30443 \
            --set controller.config.keep-alive=75 \
            --set controller.config.keep-alive-requests=1000 \
            --set controller.config.proxy-read-timeout=120 \
            --set controller.config.proxy-send-timeout=120 \
            --set controller.config.proxy-body-size=50m \
            --set controller.config.use-gzip=true \
            --set controller.config.gzip-types="text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript image/svg+xml" \
            --set controller.config.enable-brotli=true \
            --set controller.config.brotli-level=5 \
            --set controller.config.brotli-types="text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript image/svg+xml"

      - name: Install/Upgrade Metrics Server
        run: |
          helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
          helm repo update

          echo "ðŸ“¦ Mevcut Metrics Server kaynaklarÄ± Helm'e aktarÄ±lÄ±yor..."

          kubectl label clusterrole system:metrics-server app.kubernetes.io/managed-by=Helm --overwrite 2>/dev/null || true
          kubectl annotate clusterrole system:metrics-server meta.helm.sh/release-name=metrics-server --overwrite 2>/dev/null || true
          kubectl annotate clusterrole system:metrics-server meta.helm.sh/release-namespace=kube-system --overwrite 2>/dev/null || true
          
          kubectl label clusterrolebinding system:metrics-server app.kubernetes.io/managed-by=Helm --overwrite 2>/dev/null || true
          kubectl annotate clusterrolebinding system:metrics-server meta.helm.sh/release-name=metrics-server --overwrite 2>/dev/null || true
          kubectl annotate clusterrolebinding system:metrics-server meta.helm.sh/release-namespace=kube-system --overwrite 2>/dev/null || true
          
          kubectl label clusterrolebinding metrics-server:system:auth-delegator app.kubernetes.io/managed-by=Helm --overwrite 2>/dev/null || true
          kubectl annotate clusterrolebinding metrics-server:system:auth-delegator meta.helm.sh/release-name=metrics-server --overwrite 2>/dev/null || true
          kubectl annotate clusterrolebinding metrics-server:system:auth-delegator meta.helm.sh/release-namespace=kube-system --overwrite 2>/dev/null || true

          kubectl label rolebinding metrics-server-auth-reader -n kube-system app.kubernetes.io/managed-by=Helm --overwrite 2>/dev/null || true
          kubectl annotate rolebinding metrics-server-auth-reader -n kube-system meta.helm.sh/release-name=metrics-server --overwrite 2>/dev/null || true
          kubectl annotate rolebinding metrics-server-auth-reader -n kube-system meta.helm.sh/release-namespace=kube-system --overwrite 2>/dev/null || true

          kubectl label service metrics-server -n kube-system app.kubernetes.io/managed-by=Helm --overwrite 2>/dev/null || true
          kubectl annotate service metrics-server -n kube-system meta.helm.sh/release-name=metrics-server --overwrite 2>/dev/null || true
          kubectl annotate service metrics-server -n kube-system meta.helm.sh/release-namespace=kube-system --overwrite 2>/dev/null || true

          kubectl label deployment metrics-server -n kube-system app.kubernetes.io/managed-by=Helm --overwrite 2>/dev/null || true
          kubectl annotate deployment metrics-server -n kube-system meta.helm.sh/release-name=metrics-server --overwrite 2>/dev/null || true
          kubectl annotate deployment metrics-server -n kube-system meta.helm.sh/release-namespace=kube-system --overwrite 2>/dev/null || true

          kubectl label serviceaccount metrics-server -n kube-system app.kubernetes.io/managed-by=Helm --overwrite 2>/dev/null || true
          kubectl annotate serviceaccount metrics-server -n kube-system meta.helm.sh/release-name=metrics-server --overwrite 2>/dev/null || true
          kubectl annotate serviceaccount metrics-server -n kube-system meta.helm.sh/release-namespace=kube-system --overwrite 2>/dev/null || true

          kubectl label apiservice v1beta1.metrics.k8s.io app.kubernetes.io/managed-by=Helm --overwrite 2>/dev/null || true
          kubectl annotate apiservice v1beta1.metrics.k8s.io meta.helm.sh/release-name=metrics-server --overwrite 2>/dev/null || true
          kubectl annotate apiservice v1beta1.metrics.k8s.io meta.helm.sh/release-namespace=kube-system --overwrite 2>/dev/null || true

          echo "âœ… Kaynaklar Helm'e aktarÄ±ldÄ±"

          # Åžimdi Helm ile yÃ¼kle/gÃ¼ncelle
          helm upgrade --install metrics-server metrics-server/metrics-server \
            --namespace kube-system \
            --set args[0]=--kubelet-insecure-tls \
            --set args[1]=--kubelet-preferred-address-types=InternalIP \
            --set args[2]=--kubelet-preferred-address-types=ExternalIP \
            --set args[3]=--kubelet-preferred-address-types=Hostname \
            --set serviceAccount.create=false \
            --set serviceAccount.name=metrics-server
      - name: Detect Storage Class
        id: storage
        run: |
          DEFAULT_STORAGE_CLASS=$(kubectl get storageclass -o jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubernetes\.io/is-default-class=="true")].metadata.name}' 2>/dev/null || echo "")
          if [ -z "$DEFAULT_STORAGE_CLASS" ]; then
            STORAGE_CLASS=$(kubectl get storageclass -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "local-path")
          else
            STORAGE_CLASS="$DEFAULT_STORAGE_CLASS"
          fi
          echo "name=$STORAGE_CLASS" >> "$GITHUB_OUTPUT"

      - name: Install/Upgrade Prometheus Stack
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update
          helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            --create-namespace \
            --set grafana.adminPassword=admin \
            --set grafana.persistence.enabled=true \
            --set grafana.persistence.size=5Gi \
            --set grafana.persistence.storageClassName=${{ steps.storage.outputs.name }} \
            --set prometheus.prometheusSpec.retention=15d \
            --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.storageClassName=${{ steps.storage.outputs.name }} \
            --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=20Gi \
            --set alertmanager.alertmanagerSpec.storage.volumeClaimTemplate.spec.storageClassName=${{ steps.storage.outputs.name }} \
            --set alertmanager.alertmanagerSpec.storage.volumeClaimTemplate.spec.resources.requests.storage=10Gi

      - name: Verify Required Secrets
        run: |
          echo "ðŸ” Gerekli secrets kontrol ediliyor..."
          REQUIRED_SECRETS=(
            "AUTH_POSTGRES_PASSWORD"
            "PRODUCT_POSTGRES_PASSWORD"
            "JWT_SECRET"
            "AUTH_JWT_SECRET"
            "AUTH_JWT_REFRESH_SECRET"
          )
          
          MISSING_SECRETS=()
          for secret in "${REQUIRED_SECRETS[@]}"; do
            if [[ -z "${!secret}" ]]; then
              MISSING_SECRETS+=("$secret")
            fi
          done
          
          if [[ ${#MISSING_SECRETS[@]} -gt 0 ]]; then
            echo "âŒ Eksik secrets bulundu: ${MISSING_SECRETS[*]}"
            echo "LÃ¼tfen GitHub Repository Settings > Secrets and variables > Actions bÃ¶lÃ¼mÃ¼nden bu secrets'larÄ± ekleyin."
            exit 1
          fi
          
          echo "âœ… TÃ¼m gerekli secrets mevcut"
        env:
          AUTH_POSTGRES_PASSWORD: ${{ secrets.AUTH_POSTGRES_PASSWORD }}
          PRODUCT_POSTGRES_PASSWORD: ${{ secrets.PRODUCT_POSTGRES_PASSWORD }}
          JWT_SECRET: ${{ secrets.JWT_SECRET }}
          AUTH_JWT_SECRET: ${{ secrets.AUTH_JWT_SECRET }}
          AUTH_JWT_REFRESH_SECRET: ${{ secrets.AUTH_JWT_REFRESH_SECRET }}

      - name: Helm Deploy
        run: |
          cd aws-k8s-helm
          
          # values.yaml kullanÄ±lÄ±yor, hassas bilgiler GitHub Secrets'tan gelecek
          IMAGE_TAG="${{ github.sha }}"
          
          echo "ðŸš€ Helm deployment baÅŸlatÄ±lÄ±yor..."
          echo "Image Tag: ${IMAGE_TAG}"
          echo "Namespace: ${{ env.DEPLOY_ENV }}"
          
          # Metrics server uyarÄ±larÄ±nÄ± bastÄ±rmak iÃ§in
          export KUBECTL_INTERACTIVE_TIMEOUT=30s
          
          STORAGE_CLASS="${{ steps.storage.outputs.name }}"
          echo "ðŸ“¦ KullanÄ±lacak Storage Class: ${STORAGE_CLASS}"
          
          # Helm deployment - --wait olmadan, pod'lar arka planda baÅŸlatÄ±lÄ±r
          # Helm PostgreSQL'leri doÄŸru storage class ile yÃ¶netecek
          helm upgrade --install aws-k8s-helm . \
            --namespace=${{ env.DEPLOY_ENV }} \
            --values=values.yaml \
            --set global.imageRegistry=docker.io \
            --set global.storageClass="${STORAGE_CLASS}" \
            --set authApi.image.tag="${IMAGE_TAG}" \
            --set productApi.image.tag="${IMAGE_TAG}" \
            --set gateway.image.tag="${IMAGE_TAG}" \
            --set ecommerce.image.tag="${IMAGE_TAG}" \
            --set authApi.env.POSTGRES_PASSWORD="${{ secrets.AUTH_POSTGRES_PASSWORD }}" \
            --set productApi.env.POSTGRES_PASSWORD="${{ secrets.PRODUCT_POSTGRES_PASSWORD }}" \
            --set productApi.env.JWT_SECRET="${{ secrets.JWT_SECRET }}" \
            --set postgresql-auth.auth.postgresPassword="${{ secrets.AUTH_POSTGRES_PASSWORD }}" \
            --set postgresql-auth.auth.password="${{ secrets.AUTH_POSTGRES_PASSWORD }}" \
            --set postgresql-auth.primary.persistence.storageClass="${STORAGE_CLASS}" \
            --set postgresql-product.auth.postgresPassword="${{ secrets.PRODUCT_POSTGRES_PASSWORD }}" \
            --set postgresql-product.auth.password="${{ secrets.PRODUCT_POSTGRES_PASSWORD }}" \
            --set postgresql-product.primary.persistence.storageClass="${STORAGE_CLASS}" \
            --set secrets.authApi.jwtSecret="${{ secrets.AUTH_JWT_SECRET }}" \
            --set secrets.authApi.jwtRefreshSecret="${{ secrets.AUTH_JWT_REFRESH_SECRET }}" \
            --set secrets.authApi.databaseUrl="${{ secrets.AUTH_DATABASE_URL }}" \
            --set secrets.productApi.databaseUrl="${{ secrets.PRODUCT_DATABASE_URL }}" \
            --force 2>&1 | tee /tmp/helm-output.log || {
              HELM_ERROR=$(cat /tmp/helm-output.log)
              if echo "$HELM_ERROR" | grep -q "StatefulSet.*is invalid.*spec.*Forbidden"; then
                echo ""
                echo "âš ï¸  StatefulSet storage class hatasÄ± tespit edildi."
                echo "â„¹ï¸  Mevcut StatefulSet'ler farklÄ± storage class kullanÄ±yor."
                echo "â„¹ï¸  Verileri korumak iÃ§in StatefulSet'ler deÄŸiÅŸtirilmiyor."
                echo "â„¹ï¸  Sadece diÄŸer servisler gÃ¼ncelleniyor..."
                
                # StatefulSet'leri atlayarak diÄŸer servisleri gÃ¼ncelle
                helm upgrade --install aws-k8s-helm . \
                  --namespace=${{ env.DEPLOY_ENV }} \
                  --values=values.yaml \
                  --set global.imageRegistry=docker.io \
                  --set authApi.image.tag="${IMAGE_TAG}" \
                  --set productApi.image.tag="${IMAGE_TAG}" \
                  --set gateway.image.tag="${IMAGE_TAG}" \
                  --set ecommerce.image.tag="${IMAGE_TAG}" \
                  --set authApi.env.POSTGRES_PASSWORD="${{ secrets.AUTH_POSTGRES_PASSWORD }}" \
                  --set productApi.env.POSTGRES_PASSWORD="${{ secrets.PRODUCT_POSTGRES_PASSWORD }}" \
                  --set productApi.env.JWT_SECRET="${{ secrets.JWT_SECRET }}" \
                  --set postgresql-auth.enabled=false \
                  --set postgresql-product.enabled=false \
                  --set secrets.authApi.jwtSecret="${{ secrets.AUTH_JWT_SECRET }}" \
                  --set secrets.authApi.jwtRefreshSecret="${{ secrets.AUTH_JWT_REFRESH_SECRET }}" \
                  --set secrets.authApi.databaseUrl="${{ secrets.AUTH_DATABASE_URL }}" \
                  --set secrets.productApi.databaseUrl="${{ secrets.PRODUCT_DATABASE_URL }}"
                
                echo ""
                echo "âœ… DiÄŸer servisler gÃ¼ncellendi. PostgreSQL StatefulSet'leri korundu."
                echo "â„¹ï¸  PostgreSQL'leri gÃ¼ncellemek iÃ§in manuel olarak scale down/up yapabilirsiniz."
              else
                echo "âŒ Helm deployment baÅŸarÄ±sÄ±z oldu:"
                echo "$HELM_ERROR"
                exit 1
              fi
            }
          
          echo "âœ… Helm deployment tamamlandÄ±. Pod'lar arka planda baÅŸlatÄ±lÄ±yor..."

      - name: Check PVC Status
        if: always()
        run: |
          echo "## ðŸ“¦ PVC DurumlarÄ±" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          kubectl get pvc -n ${{ env.DEPLOY_ENV }} -o wide >> $GITHUB_STEP_SUMMARY || echo "PVC bulunamadÄ±" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## ðŸ” PVC DetaylarÄ±" >> $GITHUB_STEP_SUMMARY
          for pvc in $(kubectl get pvc -n ${{ env.DEPLOY_ENV }} -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo ""); do
            if [ -n "$pvc" ]; then
              echo "### PVC: $pvc" >> $GITHUB_STEP_SUMMARY
              kubectl describe pvc "$pvc" -n ${{ env.DEPLOY_ENV }} >> $GITHUB_STEP_SUMMARY 2>&1 || true
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          done

      - name: Check Pod Status
        if: always()
        run: |
          # Metrics server uyarÄ±larÄ±nÄ± bastÄ±rmak iÃ§in
          export KUBECTL_INTERACTIVE_TIMEOUT=30s
          
          echo "## ðŸ“Š Pod DurumlarÄ±" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          kubectl get pods -n ${{ env.DEPLOY_ENV }} -o wide --request-timeout=30s 2>/dev/null >> $GITHUB_STEP_SUMMARY || kubectl get pods -n ${{ env.DEPLOY_ENV }} -o wide >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## ðŸ” Son Events" >> $GITHUB_STEP_SUMMARY
          kubectl get events -n ${{ env.DEPLOY_ENV }} --sort-by='.lastTimestamp' --request-timeout=30s 2>/dev/null | tail -30 >> $GITHUB_STEP_SUMMARY || kubectl get events -n ${{ env.DEPLOY_ENV }} --sort-by='.lastTimestamp' | tail -30 >> $GITHUB_STEP_SUMMARY || true

      - name: Show Pod Logs (if failed)
        if: failure()
        run: |
          echo "## ðŸ“‹ Pod LoglarÄ± (Hata Durumunda)" >> $GITHUB_STEP_SUMMARY
          for pod in $(kubectl get pods -n ${{ env.DEPLOY_ENV }} -o jsonpath='{.items[*].metadata.name}'); do
            echo "### Pod: $pod" >> $GITHUB_STEP_SUMMARY
            kubectl logs -n ${{ env.DEPLOY_ENV }} $pod --tail=50 >> $GITHUB_STEP_SUMMARY 2>&1 || true
            echo "" >> $GITHUB_STEP_SUMMARY
          done

      - name: Verify Deployment
        run: |
          echo "## ðŸš€ Deployment Complete!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          kubectl get pods -n ${{ env.DEPLOY_ENV }} >> $GITHUB_STEP_SUMMARY
          kubectl get svc -n ${{ env.DEPLOY_ENV }} >> $GITHUB_STEP_SUMMARY
